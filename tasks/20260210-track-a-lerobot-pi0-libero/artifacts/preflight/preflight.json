{
  "backend_attempts": [
    {
      "backend": "egl",
      "message": "[robosuite WARNING] No private macro file found! (__init__.py:7)\n[robosuite WARNING] It is recommended to use a private macro file (__init__.py:8)\n[robosuite WARNING] To setup, run: python /home/ray/anaconda3/envs/vla_pi0/lib/python3.10/site-packages/robosuite/scripts/setup_macros.py (__init__.py:9)\nok egl",
      "ok": true
    }
  ],
  "checks": {
    "gpu_ids_requested_present": true,
    "lerobot_eval_help": true,
    "lerobot_import": true,
    "offscreen_backend": true,
    "policy_resolve": true
  },
  "config": {
    "artifacts_dir": "tasks/20260210-track-a-lerobot-pi0-libero/artifacts/preflight",
    "gpu_ids": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7
    ],
    "mujoco_gl": "egl",
    "policy_path": "lerobot/pi0_libero_finetuned",
    "task_suite": "libero_object"
  },
  "display_env": "",
  "generated_at_utc": "2026-02-19T18:53:50Z",
  "gpu_count": 8,
  "gpu_names": [
    "NVIDIA H100 80GB HBM3",
    "NVIDIA H100 80GB HBM3",
    "NVIDIA H100 80GB HBM3",
    "NVIDIA H100 80GB HBM3",
    "NVIDIA H100 80GB HBM3",
    "NVIDIA H100 80GB HBM3",
    "NVIDIA H100 80GB HBM3",
    "NVIDIA H100 80GB HBM3"
  ],
  "lerobot_eval_help_message": "usage: lerobot-eval [-h] [--config_path str] [--env str]\n                    [--env.type {aloha,pusht,gym_manipulator,libero,metaworld,isaaclab_arena}]\n                    [--env.visualization_width int]\n                    [--env.visualization_height int] [--robot str]\n                    [--env.robot.type {}] [--teleop str]\n                    [--env.teleop.type {}] [--processor str]\n                    [--env.processor.control_mode str] [--observation str]\n                    [--env.processor.observation.add_joint_velocity_to_observation bool]\n                    [--env.processor.observation.add_current_to_observation bool]\n                    [--env.processor.observation.add_ee_pose_to_observation bool]\n                    [--env.processor.observation.display_cameras bool]\n                    [--image_preprocessing str]\n                    [--env.processor.image_preprocessing.crop_params_dict [Dict]]\n                    [--env.processor.image_preprocessing.resize_size [int int]]\n                    [--gripper str] [--env.processor.gripper.use_gripper bool]\n                    [--env.processor.gripper.gripper_penalty float]\n                    [--reset str]\n                    [--env.processor.reset.fixed_reset_joint_positions [Any]]\n                    [--env.processor.reset.reset_time_s float]\n                    [--env.processor.reset.control_time_s float]\n                    [--env.processor.reset.terminate_on_success bool]\n                    [--inverse_kinematics str]\n                    [--env.processor.inverse_kinematics.urdf_path [str]]\n                    [--env.processor.inverse_kinematics.target_frame_name [str]]\n                    [--env.processor.inverse_kinematics.end_effector_bounds [Dict]]\n                    [--env.processor.inverse_kinematics.end_effector_step_sizes [Dict]]\n                    [--reward_classifier str]\n                    [--env.processor.reward_classifier.pretrained_path [str]]\n                    [--env.processor.reward_classifier.success_threshold float]\n                    [--env.processor.reward_classifier.success_reward float]\n                    [--env.processor.max_gripper_pos [float]] [--env.name str]\n                    [--env.task_ids [List]] [--env.camera_name str]\n                    [--env.init_states bool]\n                    [--env.camera_name_mapping [Dict]]\n                    [--env.observation_height int]\n                    [--env.observation_width int] [--env.control_mode str]\n                    [--env.obs_type str] [--env.render_mode str]\n                    [--env.multitask_eval bool] [--env.task [str]]\n                    [--env.fps int] [--env.features Dict]\n                    [--env.features_map Dict] [--env.max_parallel_tasks int]\n                    [--env.disable_env_checker bool] [--env.hub_path str]\n                    [--env.episode_length int] [--env.num_envs int]\n                    [--env.embodiment [str]] [--env.object [str]]\n                    [--env.mimic bool] [--env.teleop_device [str]]\n                    [--env.seed [int]] [--env.device [str]]\n                    [--env.disable_fabric bool] [--env.enable_cameras bool]\n                    [--env.headless bool] [--env.enable_pinocchio bool]\n                    [--env.environment [str]] [--env.state_dim int]\n                    [--env.action_dim int] [--env.camera_height int]\n                    [--env.camera_width int] [--env.video bool]\n                    [--env.video_length int] [--env.video_interval int]\n                    [--env.state_keys str] [--env.camera_keys [str]]\n                    [--env.kwargs [dict]] [--eval str] [--eval.n_episodes int]\n                    [--eval.batch_size int] [--eval.use_async_envs bool]\n                    [--policy str]\n                    [--policy.type {act,diffusion,groot,pi0,pi0_fast,pi05,smolvla,tdmpc,vqbet,wall_x,xvla,sac,reward_classifier,sarm}]\n                    [--policy.replace_final_stride_with_dilation int]\n                    [--policy.pre_norm bool] [--policy.dim_model int]\n                    [--policy.n_heads int] [--policy.dim_feedforward int]\n                    [--policy.feedforward_activation str]\n                    [--policy.n_encoder_layers int]\n                    [--policy.n_decoder_layers int] [--policy.use_vae bool]\n                    [--policy.n_vae_encoder_layers int]\n                    [--policy.temporal_ensemble_coeff [float]]\n                    [--policy.kl_weight float]\n                    [--policy.optimizer_lr_backbone float]\n                    [--policy.use_separate_rgb_encoder_per_camera bool]\n                    [--policy.down_dims int [int, ...]]\n                    [--policy.kernel_size int] [--policy.n_groups int]\n                    [--policy.diffusion_step_embed_dim int]\n                    [--policy.use_film_scale_modulation bool]\n                    [--policy.noise_scheduler_type str]\n                    [--policy.num_train_timesteps int]\n                    [--policy.beta_schedule str] [--policy.beta_start float]\n                    [--policy.beta_end float] [--policy.prediction_type str]\n                    [--policy.clip_sample bool]\n                    [--policy.clip_sample_range float]\n                    [--policy.do_mask_loss_for_padding bool]\n                    [--policy.scheduler_name str]\n                    [--policy.image_size int int]\n                    [--policy.base_model_path str]\n                    [--policy.tokenizer_assets_repo str]\n                    [--policy.embodiment_tag str] [--policy.tune_llm bool]\n                    [--policy.tune_visual bool] [--policy.tune_projector bool]\n                    [--policy.tune_diffusion_model bool]\n                    [--policy.lora_rank int] [--policy.lora_alpha int]\n                    [--policy.lora_dropout float]\n                    [--policy.lora_full_model bool]\n                    [--policy.warmup_ratio float] [--policy.use_bf16 bool]\n                    [--policy.video_backend str]\n                    [--policy.balance_dataset_weights bool]\n                    [--policy.balance_trajectory_weights bool]\n                    [--policy.dataset_paths [List]] [--policy.output_dir str]\n                    [--policy.save_steps int] [--policy.max_steps int]\n                    [--policy.dataloader_num_workers int]\n                    [--policy.report_to str] [--policy.resume bool]\n                    [--policy.max_action_tokens int]\n                    [--policy.text_tokenizer_name str]\n                    [--policy.action_tokenizer_name str]\n                    [--policy.temperature float]\n                    [--policy.max_decoding_steps int]\n                    [--policy.fast_skip_tokens int]\n                    [--policy.validate_action_token_prefix bool]\n                    [--policy.use_kv_cache bool]\n                    [--policy.paligemma_variant str]\n                    [--policy.action_expert_variant str]\n                    [--policy.num_inference_steps int]\n                    [--policy.time_sampling_beta_alpha float]\n                    [--policy.time_sampling_beta_beta float]\n                    [--policy.time_sampling_scale float]\n                    [--policy.time_sampling_offset float]\n                    [--policy.image_resolution int int]\n                    [--policy.gradient_checkpointing bool]\n                    [--policy.compile_model bool] [--policy.compile_mode str]\n                    [--policy.adapt_to_pi_aloha bool]\n                    [--policy.use_delta_joint_actions_aloha bool]\n                    [--policy.num_steps int] [--policy.use_cache bool]\n                    [--policy.train_expert_only bool]\n                    [--policy.train_state_proj bool]\n                    [--policy.vlm_model_name str]\n                    [--policy.load_vlm_weights bool]\n                    [--policy.add_image_special_tokens bool]\n                    [--policy.attention_mode str] [--policy.prefix_length int]\n                    [--policy.num_expert_layers int]\n                    [--policy.num_vlm_layers int]\n                    [--policy.self_attn_every_n_layers int]\n                    [--policy.expert_width_multiplier float]\n                    [--policy.min_period float] [--policy.max_period float]\n                    [--rtc_config str] [--policy.rtc_config.enabled bool]\n                    [--policy.rtc_config.prefix_attention_schedule RTCAttentionSchedule]\n                    [--policy.rtc_config.max_guidance_weight float]\n                    [--policy.rtc_config.execution_horizon int]\n                    [--policy.rtc_config.debug bool]\n                    [--policy.rtc_config.debug_maxlen int]\n                    [--policy.n_action_repeats int] [--policy.horizon int]\n                    [--policy.q_ensemble_size int] [--policy.mlp_dim int]\n                    [--policy.use_mpc bool] [--policy.cem_iterations int]\n                    [--policy.max_std float] [--policy.min_std float]\n                    [--policy.n_gaussian_samples int]\n                    [--policy.n_pi_samples int]\n                    [--policy.uncertainty_regularizer_coeff float]\n                    [--policy.n_elites int]\n                    [--policy.elite_weighting_temperature float]\n                    [--policy.gaussian_mean_momentum float]\n                    [--policy.max_random_shift_ratio float]\n                    [--policy.reward_coeff float]\n                    [--policy.expectile_weight float]\n                    [--policy.value_coeff float]\n                    [--policy.consistency_coeff float]\n                    [--policy.advantage_scaling float]\n                    [--policy.pi_coeff float]\n                    [--policy.temporal_decay_coeff float]\n                    [--policy.target_model_momentum float]\n                    [--policy.n_action_pred_token int]\n                    [--policy.action_chunk_size int]\n                    [--policy.vision_backbone str]\n                    [--policy.crop_shape [int int]]\n                    [--policy.crop_is_random bool]\n                    [--policy.pretrained_backbone_weights [str]]\n                    [--policy.use_group_norm bool]\n                    [--policy.spatial_softmax_num_keypoints int]\n                    [--policy.n_vqvae_training_steps int]\n                    [--policy.vqvae_n_embed int]\n                    [--policy.vqvae_embedding_dim int]\n                    [--policy.vqvae_enc_hidden_dim int]\n                    [--policy.gpt_block_size int] [--policy.gpt_input_dim int]\n                    [--policy.gpt_output_dim int] [--policy.gpt_n_layer int]\n                    [--policy.gpt_n_head int] [--policy.gpt_hidden_dim int]\n                    [--policy.offset_loss_weight float]\n                    [--policy.primary_code_loss_weight float]\n                    [--policy.secondary_code_loss_weight float]\n                    [--policy.bet_softmax_temperature float]\n                    [--policy.sequentially_select bool]\n                    [--policy.optimizer_vqvae_lr float]\n                    [--policy.optimizer_vqvae_weight_decay float]\n                    [--policy.pretrained_name_or_path str]\n                    [--policy.action_tokenizer_path [str]]\n                    [--policy.prediction_mode str]\n                    [--policy.attn_implementation str]\n                    [--policy.chunk_size int] [--policy.n_action_steps int]\n                    [--policy.dtype str] [--policy.florence_config Dict]\n                    [--policy.tokenizer_name str]\n                    [--policy.tokenizer_max_length int]\n                    [--policy.tokenizer_padding_side str]\n                    [--policy.pad_language_to str] [--policy.hidden_size int]\n                    [--policy.depth int] [--policy.mlp_ratio float]\n                    [--policy.num_domains int] [--policy.len_soft_prompts int]\n                    [--policy.dim_time int] [--policy.max_len_seq int]\n                    [--policy.use_hetero_proj bool] [--policy.action_mode str]\n                    [--policy.num_denoising_steps int]\n                    [--policy.use_proprio bool] [--policy.max_action_dim int]\n                    [--policy.domain_feature_key [str]]\n                    [--policy.resize_imgs_with_padding [int int]]\n                    [--policy.num_image_views [int]]\n                    [--policy.empty_cameras int]\n                    [--policy.freeze_language_encoder bool]\n                    [--policy.train_policy_transformer bool]\n                    [--policy.train_soft_prompts bool]\n                    [--policy.optimizer_lr float]\n                    [--policy.optimizer_betas float float]\n                    [--policy.optimizer_eps float]\n                    [--policy.optimizer_weight_decay float]\n                    [--policy.optimizer_grad_clip_norm float]\n                    [--policy.optimizer_soft_prompt_lr_scale float]\n                    [--policy.optimizer_soft_prompt_warmup_lr_scale [float]]\n                    [--policy.scheduler_warmup_steps int]\n                    [--policy.scheduler_decay_steps int]\n                    [--policy.scheduler_decay_lr float]\n                    [--policy.dataset_stats [Dict]]\n                    [--policy.storage_device str]\n                    [--policy.vision_encoder_name [str]]\n                    [--policy.freeze_vision_encoder bool]\n                    [--policy.image_encoder_hidden_dim int]\n                    [--policy.shared_encoder bool]\n                    [--policy.num_discrete_actions [int]]\n                    [--policy.online_steps int]\n                    [--policy.online_buffer_capacity int]\n                    [--policy.offline_buffer_capacity int]\n                    [--policy.async_prefetch bool]\n                    [--policy.online_step_before_learning int]\n                    [--policy.policy_update_freq int]\n                    [--policy.discount float]\n                    [--policy.temperature_init float]\n                    [--policy.num_critics int]\n                    [--policy.num_subsample_critics [int]]\n                    [--policy.critic_lr float] [--policy.actor_lr float]\n                    [--policy.temperature_lr float]\n                    [--policy.critic_target_update_weight float]\n                    [--policy.utd_ratio int]\n                    [--policy.state_encoder_hidden_dim int]\n                    [--policy.target_entropy [float]]\n                    [--policy.use_backup_entropy bool]\n                    [--critic_network_kwargs str]\n                    [--policy.critic_network_kwargs.hidden_dims List]\n                    [--policy.critic_network_kwargs.activate_final bool]\n                    [--policy.critic_network_kwargs.final_activation [str]]\n                    [--actor_network_kwargs str]\n                    [--policy.actor_network_kwargs.hidden_dims List]\n                    [--policy.actor_network_kwargs.activate_final bool]\n                    [--policy_kwargs str]\n                    [--policy.policy_kwargs.use_tanh_squash bool]\n                    [--policy.policy_kwargs.std_min float]\n                    [--policy.policy_kwargs.std_max float]\n                    [--policy.policy_kwargs.init_final float]\n                    [--discrete_critic_network_kwargs str]\n                    [--policy.discrete_critic_network_kwargs.hidden_dims List]\n                    [--policy.discrete_critic_network_kwargs.activate_final bool]\n                    [--policy.discrete_critic_network_kwargs.final_activation [str]]\n                    [--actor_learner_config str]\n                    [--policy.actor_learner_config.learner_host str]\n                    [--policy.actor_learner_config.learner_port int]\n                    [--policy.actor_learner_config.policy_parameters_push_frequency int]\n                    [--policy.actor_learner_config.queue_get_timeout float]\n                    [--concurrency str] [--policy.concurrency.actor str]\n                    [--policy.concurrency.learner str]\n                    [--policy.use_torch_compile bool] [--policy.name str]\n                    [--policy.num_classes int] [--policy.latent_dim int]\n                    [--policy.image_embedding_pooling_dim int]\n                    [--policy.dropout_rate float] [--policy.model_name str]\n                    [--policy.model_type str] [--policy.num_cameras int]\n                    [--policy.learning_rate float]\n                    [--policy.weight_decay float]\n                    [--policy.grad_clip_norm float] [--policy.n_obs_steps int]\n                    [--policy.input_features dict]\n                    [--policy.output_features dict] [--policy.device [str]]\n                    [--policy.use_amp bool] [--policy.use_peft bool]\n                    [--policy.push_to_hub bool] [--policy.repo_id [str]]\n                    [--policy.private [bool]] [--policy.tags [List]]\n                    [--policy.license [str]] [--policy.pretrained_path [Path]]\n                    [--policy.annotation_mode str] [--policy.frame_gap int]\n                    [--policy.max_rewind_steps int] [--policy.image_dim int]\n                    [--policy.text_dim int] [--policy.hidden_dim int]\n                    [--policy.num_heads int] [--policy.num_layers int]\n                    [--policy.max_state_dim int]\n                    [--policy.drop_n_last_frames int]\n                    [--policy.batch_size int] [--policy.clip_batch_size int]\n                    [--policy.dropout float]\n                    [--policy.stage_loss_weight float]\n                    [--policy.rewind_probability float]\n                    [--policy.language_perturbation_probability float]\n                    [--policy.num_sparse_stages int]\n                    [--policy.sparse_subtask_names [list]]\n                    [--policy.sparse_temporal_proportions [list]]\n                    [--policy.num_dense_stages [int]]\n                    [--policy.dense_subtask_names [list]]\n                    [--policy.dense_temporal_proportions [list]]\n                    [--policy.pretrained_model_path [str]]\n                    [--policy.image_key str] [--policy.state_key str]\n                    [--policy.normalization_mapping Dict]\n                    [--output_dir [Path]] [--job_name [str]] [--seed [int]]\n                    [--rename_map Dict] [--trust_remote_code bool]\n\noptions:\n  -h, --help            show this help message and exit\n  --config_path str     Path for a config file to parse with draccus (default:\n                        None)\n  --env str             Config file for env (default: None)\n  --robot str           Config file for robot (default: None)\n  --teleop str          Config file for teleop (default: None)\n  --processor str       Config file for processor (default: None)\n  --observation str     Config file for observation (default: None)\n  --image_preprocessing str\n                        Config file for image_preprocessing (default: None)\n  --gripper str         Config file for gripper (default: None)\n  --reset str           Config file for reset (default: None)\n  --inverse_kinematics str\n                        Config file for inverse_kinematics (default: None)\n  --reward_classifier str\n                        Config file for reward_classifier (default: None)\n  --eval str            Config file for eval (default: None)\n  --policy str          Config file for policy (default: None)\n  --rtc_config str      Config file for rtc_config (default: None)\n  --rtc_config str      Config file for rtc_config (default: None)\n  --rtc_config str      Config file for rtc_config (default: None)\n  --rtc_config str      Config file for rtc_config (default: None)\n  --critic_network_kwargs str\n                        Config file for critic_network_kwargs (default: None)\n  --actor_network_kwargs str\n                        Config file for actor_network_kwargs (default: None)\n  --policy_kwargs str   Config file for policy_kwargs (default: None)\n  --discrete_critic_network_kwargs str\n                        Config file for discrete_critic_network_kwargs\n                        (default: None)\n  --actor_learner_config str\n                        Config file for actor_learner_config (default: None)\n  --concurrency str     Config file for concurrency (default: None)\n\nEvalPipelineConfig:\n\n  --output_dir [Path]\n  --job_name [str]\n  --seed [int]          \n  --rename_map Dict     Rename map for the observation to override the image\n                        and state keys (default: {})\n  --trust_remote_code bool\n                        Explicit consent to execute remote code from the Hub\n                        (required for hub environments). (default: False)\n\nEnvConfig ['env']:\n  Either the repo ID of a model hosted on the Hub or a path to a directory containing weights\n  saved using `Policy.save_pretrained`. If not provided, the policy is initialized from scratch\n  (useful for debugging). This argument is mutually exclusive with `--config`.\n\n  --env.type {aloha,pusht,gym_manipulator,libero,metaworld,isaaclab_arena}\n                        Which type of EnvConfig ['env'] to use (default: None)\n\nAlohaEnv ['env']:\n  Either the repo ID of a model hosted on the Hub or a path to a directory containing weights\n  saved using `Policy.save_pretrained`. If not provided, the policy is initialized from scratch\n  (useful for debugging). This argument is mutually exclusive with `--config`.\n\n  --env.task [str]      \n  --env.fps int         \n  --env.features Dict   \n  --env.features_map Dict\n  --env.max_parallel_tasks int\n  --env.disable_env_checker bool\n  --env.episode_length int\n  --env.obs_type str    \n  --env.observation_height int\n  --env.observation_width int\n  --env.render_mode str\n\nPushtEnv ['env']:\n  Either the repo ID of a model hosted on the Hub or a path to a directory containing weights\n  saved using `Policy.save_pretrained`. If not provided, the policy is initialized from scratch\n  (useful for debugging). This argument is mutually exclusive with `--config`.\n\n  --env.task [str]      \n  --env.fps int         \n  --env.features Dict   \n  --env.features_map Dict\n  --env.max_parallel_tasks int\n  --env.disable_env_checker bool\n  --env.episode_length int\n  --env.obs_type str    \n  --env.render_mode str\n  --env.visualization_width int\n  --env.visualization_height int\n  --env.observation_height int\n  --env.observation_width int\n\nHILSerlRobotEnvConfig ['env']:\n  Either the repo ID of a model hosted on the Hub or a path to a directory containing weights\n  saved using `Policy.save_pretrained`. If not provided, the policy is initialized from scratch\n  (useful for debugging). This argument is mutually exclusive with `--config`.\n\n  --env.task [str]\n  --env.fps int         \n  --env.features Dict   \n  --env.features_map Dict\n  --env.max_parallel_tasks int\n  --env.disable_env_checker bool\n  --env.name str        \n\nOptional ['env.robot']:\n\nRobotConfig ['env.robot']:\n\n  --env.robot.type {}   Which type of RobotConfig ['env.robot'] to use\n                        (default: None)\n\nOptional ['env.teleop']:\n\nTeleoperatorConfig ['env.teleop']:\n\n  --env.teleop.type {}  Which type of TeleoperatorConfig ['env.teleop'] to use\n                        (default: None)\n\nHILSerlProcessorConfig ['env.processor']:\n  Configuration for environment processing pipeline.\n\n  --env.processor.control_mode str\n  --env.processor.max_gripper_pos [float]\n\nOptional ['env.processor.observation']:\n\nObservationConfig ['env.processor.observation']:\n  Configuration for observation processing.\n\n  --env.processor.observation.add_joint_velocity_to_observation bool\n  --env.processor.observation.add_current_to_observation bool\n  --env.processor.observation.add_ee_pose_to_observation bool\n  --env.processor.observation.display_cameras bool\n\nOptional ['env.processor.image_preprocessing']:\n\nImagePreprocessingConfig ['env.processor.image_preprocessing']:\n\n  --env.processor.image_preprocessing.crop_params_dict [Dict]\n  --env.processor.image_preprocessing.resize_size [int int]\n\nOptional ['env.processor.gripper']:\n\nGripperConfig ['env.processor.gripper']:\n  Configuration for gripper control and penalties.\n\n  --env.processor.gripper.use_gripper bool\n  --env.processor.gripper.gripper_penalty float\n\nOptional ['env.processor.reset']:\n\nResetConfig ['env.processor.reset']:\n  Configuration for environment reset behavior.\n\n  --env.processor.reset.fixed_reset_joint_positions [Any]\n  --env.processor.reset.reset_time_s float\n  --env.processor.reset.control_time_s float\n  --env.processor.reset.terminate_on_success bool\n\nOptional ['env.processor.inverse_kinematics']:\n\nInverseKinematicsConfig ['env.processor.inverse_kinematics']:\n  Configuration for inverse kinematics processing.\n\n  --env.processor.inverse_kinematics.urdf_path [str]\n  --env.processor.inverse_kinematics.target_frame_name [str]\n  --env.processor.inverse_kinematics.end_effector_bounds [Dict]\n  --env.processor.inverse_kinematics.end_effector_step_sizes [Dict]\n\nOptional ['env.processor.reward_classifier']:\n\nRewardClassifierConfig ['env.processor.reward_classifier']:\n  Configuration for reward classification.\n\n  --env.processor.reward_classifier.pretrained_path [str]\n  --env.processor.reward_classifier.success_threshold float\n  --env.processor.reward_classifier.success_reward float\n\nLiberoEnv ['env']:\n  Either the repo ID of a model hosted on the Hub or a path to a directory containing weights\n  saved using `Policy.save_pretrained`. If not provided, the policy is initialized from scratch\n  (useful for debugging). This argument is mutually exclusive with `--config`.\n\n  --env.task str        can also choose libero_spatial, libero_object, etc.\n                        (default: libero_10)\n  --env.fps int         \n  --env.features Dict   \n  --env.features_map Dict\n  --env.max_parallel_tasks int\n  --env.disable_env_checker bool\n  --env.task_ids [List]\n  --env.episode_length [int]\n  --env.obs_type str    \n  --env.render_mode str\n  --env.camera_name str\n  --env.init_states bool\n  --env.camera_name_mapping [Dict]\n  --env.observation_height int\n  --env.observation_width int\n  --env.control_mode str\n                        or \"absolute\" (default: relative)\n\nMetaworldEnv ['env']:\n  Either the repo ID of a model hosted on the Hub or a path to a directory containing weights\n  saved using `Policy.save_pretrained`. If not provided, the policy is initialized from scratch\n  (useful for debugging). This argument is mutually exclusive with `--config`.\n\n  --env.task str        add all tasks (default: metaworld-push-v2)\n  --env.fps int         \n  --env.features Dict   \n  --env.features_map Dict\n  --env.max_parallel_tasks int\n  --env.disable_env_checker bool\n  --env.episode_length int\n  --env.obs_type str    \n  --env.render_mode str\n  --env.multitask_eval bool\n\nIsaaclabArenaEnv ['env']:\n  Either the repo ID of a model hosted on the Hub or a path to a directory containing weights\n  saved using `Policy.save_pretrained`. If not provided, the policy is initialized from scratch\n  (useful for debugging). This argument is mutually exclusive with `--config`.\n\n  --env.task [str]      \n  --env.fps int         \n  --env.features Dict   \n  --env.features_map Dict\n  --env.max_parallel_tasks int\n  --env.disable_env_checker bool\n  --env.hub_path str    \n  --env.episode_length int\n  --env.num_envs int    \n  --env.embodiment [str]\n  --env.object [str]    \n  --env.mimic bool      \n  --env.teleop_device [str]\n  --env.seed [int]      \n  --env.device [str]    \n  --env.disable_fabric bool\n  --env.enable_cameras bool\n  --env.headless bool   \n  --env.enable_pinocchio bool\n  --env.environment [str]\n  --env.state_dim int   \n  --env.action_dim int  \n  --env.camera_height int\n  --env.camera_width int\n  --env.video bool      \n  --env.video_length int\n  --env.video_interval int\n  --env.state_keys str  Comma-separated keys, e.g.,\n                        \"robot_joint_pos,left_eef_pos\" (default:\n                        robot_joint_pos)\n  --env.camera_keys [str]\n                        Comma-separated keys, e.g.,\n                        \"robot_pov_cam_rgb,front_cam_rgb\" Set to None or \"\"\n                        for environments without cameras (default: None)\n  --env.kwargs [dict]\n\nEvalConfig ['eval']:\n\n  --eval.n_episodes int\n  --eval.batch_size int\n                        `batch_size` specifies the number of environments to\n                        use in a gym.vector.VectorEnv. (default: 50)\n  --eval.use_async_envs bool\n                        `use_async_envs` specifies whether to use asynchronous\n                        environments (multiprocessing). (default: False)\n\nOptional ['policy']:\n\nPreTrainedConfig ['policy']:\n  \n      Base configuration class for policy models.\n  \n      Args:\n          n_obs_steps: Number of environment steps worth of observations to pass to the policy (takes the\n              current step and additional steps going back).\n          input_features: A dictionary defining the PolicyFeature of the input data for the policy. The key represents\n              the input data name, and the value is PolicyFeature, which consists of FeatureType and shape attributes.\n          output_features: A dictionary defining the PolicyFeature of the output data for the policy. The key represents\n              the output data name, and the value is PolicyFeature, which consists of FeatureType and shape attributes.\n          normalization_mapping: A dictionary that maps from a str value of FeatureType (e.g., \"STATE\", \"VISUAL\") to\n              a corresponding NormalizationMode (e.g., NormalizationMode.MIN_MAX)\n      \n\n  --policy.type {act,diffusion,groot,pi0,pi0_fast,pi05,smolvla,tdmpc,vqbet,wall_x,xvla,sac,reward_classifier,sarm}\n                        Which type of PreTrainedConfig ['policy'] to use\n                        (default: None)\n\nACTConfig ['policy']:\n  Configuration class for the Action Chunking Transformers policy.\n  \n      Defaults are configured for training on bimanual Aloha tasks like \"insertion\" or \"transfer\".\n  \n      The parameters you will most likely need to change are the ones which depend on the environment / sensors.\n      Those are: `input_features` and `output_features`.\n  \n      Notes on the inputs and outputs:\n          - Either:\n              - At least one key starting with \"observation.image is required as an input.\n                AND/OR\n              - The key \"observation.environment_state\" is required as input.\n          - If there are multiple keys beginning with \"observation.images.\" they are treated as multiple camera\n            views. Right now we only support all images having the same shape.\n          - May optionally work without an \"observation.state\" key for the proprioceptive robot state.\n          - \"action\" is required as an output key.\n  \n      Args:\n          n_obs_steps: Number of environment steps worth of observations to pass to the policy (takes the\n              current step and additional steps going back).\n          chunk_size: The size of the action prediction \"chunks\" in units of environment steps.\n          n_action_steps: The number of action steps to run in the environment for one invocation of the policy.\n              This should be no greater than the chunk size. For example, if the chunk size size 100, you may\n              set this to 50. This would mean that the model predicts 100 steps worth of actions, runs 50 in the\n              environment, and throws the other 50 out.\n          input_features: A dictionary defining the PolicyFeature of the input data for the policy. The key represents\n              the input data name, and the value is PolicyFeature, which consists of FeatureType and shape attributes.\n          output_features: A dictionary defining the PolicyFeature of the output data for the policy. The key represents\n              the output data name, and the value is PolicyFeature, which consists of FeatureType and shape attributes.\n          normalization_mapping: A dictionary that maps from a str value of FeatureType (e.g., \"STATE\", \"VISUAL\") to\n              a corresponding NormalizationMode (e.g., NormalizationMode.MIN_MAX)\n          vision_backbone: Name of the torchvision resnet backbone to use for encoding images.\n          pretrained_backbone_weights: Pretrained weights from torchvision to initialize the backbone.\n              `None` means no pretrained weights.\n          replace_final_stride_with_dilation: Whether to replace the ResNet's final 2x2 stride with a dilated\n              convolution.\n          pre_norm: Whether to use \"pre-norm\" in the transformer blocks.\n          dim_model: The transformer blocks' main hidden dimension.\n          n_heads: The number of heads to use in the transformer blocks' multi-head attention.\n          dim_feedforward: The dimension to expand the transformer's hidden dimension to in the feed-forward\n              layers.\n          feedforward_activation: The activation to use in the transformer block's feed-forward layers.\n          n_encoder_layers: The number of transformer layers to use for the transformer encoder.\n          n_decoder_layers: The number of transformer layers to use for the transformer decoder.\n          use_vae: Whether to use a variational objective during training. This introduces another transformer\n              which is used as the VAE's encoder (not to be confused with the transformer encoder - see\n              documentation in the policy class).\n          latent_dim: The VAE's latent dimension.\n          n_vae_encoder_layers: The number of transformer layers to use for the VAE's encoder.\n          temporal_ensemble_coeff: Coefficient for the exponential weighting scheme to apply for temporal\n              ensembling. Defaults to None which means temporal ensembling is not used. `n_action_steps` must be\n              1 when using this feature, as inference needs to happen at every step to form an ensemble. For\n              more information on how ensembling works, please see `ACTTemporalEnsembler`.\n          dropout: Dropout to use in the transformer layers (see code for details).\n          kl_weight: The weight to use for the KL-divergence component of the loss if the variational objective\n              is enabled. Loss is then calculated as: `reconstruction_loss + kl_weight * kld_loss`.\n      \n\n  --policy.n_obs_steps int\n  --policy.input_features [Dict]\n  --policy.output_features [Dict]\n  --policy.device [str]\n                        e.g. \"cuda\", \"cuda:0\", \"cpu\", or \"mps\" (default: None)\n  --policy.use_amp bool\n                        `use_amp` determines whether to use Automatic Mixed\n                        Precision (AMP) for training and evaluation. With AMP,\n                        automatic gradient scaling is used. (default: False)\n  --policy.use_peft bool\n                        Whether the policy employed PEFT for training.\n                        (default: False)\n  --policy.push_to_hub bool\n                        type: ignore[assignment] # TODO: use a different name\n                        to avoid override (default: True)\n  --policy.repo_id [str]\n  --policy.private [bool]\n                        Upload on private repository on the Hugging Face hub.\n                        (default: None)\n  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n  --policy.license [str]\n                        Add tags to your policy on the hub. (default: None)\n  --policy.pretrained_path [Path]\n                        Either the repo ID of a model hosted on the Hub or a\n                        path to a directory containing weights saved using\n                        `Policy.save_pretrained`. If not provided, the policy\n                        is initialized from scratch. (default: None)\n  --policy.chunk_size int\n  --policy.n_action_steps int\n  --policy.normalization_mapping Dict\n  --policy.vision_backbone str\n  --policy.pretrained_backbone_weights [str]\n  --policy.replace_final_stride_with_dilation int\n  --policy.pre_norm bool\n  --policy.dim_model int\n  --policy.n_heads int  \n  --policy.dim_feedforward int\n  --policy.feedforward_activation str\n  --policy.n_encoder_layers int\n  --policy.n_decoder_layers int\n  --policy.use_vae bool\n  --policy.latent_dim int\n  --policy.n_vae_encoder_layers int\n  --policy.temporal_ensemble_coeff [float]\n  --policy.dropout float\n  --policy.kl_weight float\n  --policy.optimizer_lr float\n                        Training preset (default: 1e-05)\n  --policy.optimizer_weight_decay float\n  --policy.optimizer_lr_backbone float\n\nDiffusionConfig ['policy']:\n  Configuration class for DiffusionPolicy.\n  \n      Defaults are configured for training with PushT providing proprioceptive and single camera observations.\n  \n      The parameters you will most likely need to change are the ones which depend on the environment / sensors.\n      Those are: `input_features` and `output_features`.\n  \n      Notes on the inputs and outputs:\n          - \"observation.state\" is required as an input key.\n          - Either:\n              - At least one key starting with \"observation.image is required as an input.\n                AND/OR\n              - The key \"observation.environment_state\" is required as input.\n          - If there are multiple keys beginning with \"observation.image\" they are treated as multiple camera\n            views. Right now we only support all images having the same shape.\n          - \"action\" is required as an output key.\n  \n      Args:\n          n_obs_steps: Number of environment steps worth of observations to pass to the policy (takes the\n              current step and additional steps going back).\n          horizon: Diffusion model action prediction size as detailed in `DiffusionPolicy.select_action`.\n          n_action_steps: The number of action steps to run in the environment for one invocation of the policy.\n              See `DiffusionPolicy.select_action` for more details.\n          input_features: A dictionary defining the PolicyFeature of the input data for the policy. The key represents\n              the input data name, and the value is PolicyFeature, which consists of FeatureType and shape attributes.\n          output_features: A dictionary defining the PolicyFeature of the output data for the policy. The key represents\n              the output data name, and the value is PolicyFeature, which consists of FeatureType and shape attributes.\n          normalization_mapping: A dictionary that maps from a str value of FeatureType (e.g., \"STATE\", \"VISUAL\") to\n              a corresponding NormalizationMode (e.g., NormalizationMode.MIN_MAX)\n          vision_backbone: Name of the torchvision resnet backbone to use for encoding images.\n          crop_shape: (H, W) shape to crop images to as a preprocessing step for the vision backbone. Must fit\n              within the image size. If None, no cropping is done.\n          crop_is_random: Whether the crop should be random at training time (it's always a center crop in eval\n              mode).\n          pretrained_backbone_weights: Pretrained weights from torchvision to initialize the backbone.\n              `None` means no pretrained weights.\n          use_group_norm: Whether to replace batch normalization with group normalization in the backbone.\n              The group sizes are set to be about 16 (to be precise, feature_dim // 16).\n          spatial_softmax_num_keypoints: Number of keypoints for SpatialSoftmax.\n          use_separate_rgb_encoder_per_camera: Whether to use a separate RGB encoder for each camera view.\n          down_dims: Feature dimension for each stage of temporal downsampling in the diffusion modeling Unet.\n              You may provide a variable number of dimensions, therefore also controlling the degree of\n              downsampling.\n          kernel_size: The convolutional kernel size of the diffusion modeling Unet.\n          n_groups: Number of groups used in the group norm of the Unet's convolutional blocks.\n          diffusion_step_embed_dim: The Unet is conditioned on the diffusion timestep via a small non-linear\n              network. This is the output dimension of that network, i.e., the embedding dimension.\n          use_film_scale_modulation: FiLM (https://huggingface.co/papers/1709.07871) is used for the Unet conditioning.\n              Bias modulation is used be default, while this parameter indicates whether to also use scale\n              modulation.\n          noise_scheduler_type: Name of the noise scheduler to use. Supported options: [\"DDPM\", \"DDIM\"].\n          num_train_timesteps: Number of diffusion steps for the forward diffusion schedule.\n          beta_schedule: Name of the diffusion beta schedule as per DDPMScheduler from Hugging Face diffusers.\n          beta_start: Beta value for the first forward-diffusion step.\n          beta_end: Beta value for the last forward-diffusion step.\n          prediction_type: The type of prediction that the diffusion modeling Unet makes. Choose from \"epsilon\"\n              or \"sample\". These have equivalent outcomes from a latent variable modeling perspective, but\n              \"epsilon\" has been shown to work better in many deep neural network settings.\n          clip_sample: Whether to clip the sample to [-`clip_sample_range`, +`clip_sample_range`] for each\n              denoising step at inference time. WARNING: you will need to make sure your action-space is\n              normalized to fit within this range.\n          clip_sample_range: The magnitude of the clipping range as described above.\n          num_inference_steps: Number of reverse diffusion steps to use at inference time (steps are evenly\n              spaced). If not provided, this defaults to be the same as `num_train_timesteps`.\n          do_mask_loss_for_padding: Whether to mask the loss when there are copy-padded actions. See\n              `LeRobotDataset` and `load_previous_and_future_frames` for more information. Note, this defaults\n              to False as the original Diffusion Policy implementation does the same.\n      \n\n  --policy.n_obs_steps int\n  --policy.input_features [Dict]\n  --policy.output_features [Dict]\n  --policy.device [str]\n                        e.g. \"cuda\", \"cuda:0\", \"cpu\", or \"mps\" (default: None)\n  --policy.use_amp bool\n                        `use_amp` determines whether to use Automatic Mixed\n                        Precision (AMP) for training and evaluation. With AMP,\n                        automatic gradient scaling is used. (default: False)\n  --policy.use_peft bool\n                        Whether the policy employed PEFT for training.\n                        (default: False)\n  --policy.push_to_hub bool\n                        type: ignore[assignment] # TODO: use a different name\n                        to avoid override (default: True)\n  --policy.repo_id [str]\n  --policy.private [bool]\n                        Upload on private repository on the Hugging Face hub.\n                        (default: None)\n  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n  --policy.license [str]\n                        Add tags to your policy on the hub. (default: None)\n  --policy.pretrained_path [Path]\n                        Either the repo ID of a model hosted on the Hub or a\n                        path to a directory containing weights saved using\n                        `Policy.save_pretrained`. If not provided, the policy\n                        is initialized from scratch. (default: None)\n  --policy.horizon int  \n  --policy.n_action_steps int\n  --policy.normalization_mapping Dict\n  --policy.drop_n_last_frames int\n                        horizon - n_action_steps - n_obs_steps + 1 (default:\n                        7)\n  --policy.vision_backbone str\n  --policy.crop_shape [int int]\n  --policy.crop_is_random bool\n  --policy.pretrained_backbone_weights [str]\n  --policy.use_group_norm bool\n  --policy.spatial_softmax_num_keypoints int\n  --policy.use_separate_rgb_encoder_per_camera bool\n  --policy.down_dims int [int, ...]\n  --policy.kernel_size int\n  --policy.n_groups int\n  --policy.diffusion_step_embed_dim int\n  --policy.use_film_scale_modulation bool\n  --policy.noise_scheduler_type str\n                        Noise scheduler. (default: DDPM)\n  --policy.num_train_timesteps int\n  --policy.beta_schedule str\n  --policy.beta_start float\n  --policy.beta_end float\n  --policy.prediction_type str\n  --policy.clip_sample bool\n  --policy.clip_sample_range float\n  --policy.num_inference_steps [int]\n  --policy.do_mask_loss_for_padding bool\n  --policy.optimizer_lr float\n                        Training presets (default: 0.0001)\n  --policy.optimizer_betas Any\n  --policy.optimizer_eps float\n  --policy.optimizer_weight_decay float\n  --policy.scheduler_name str\n  --policy.scheduler_warmup_steps int\n\nGrootConfig ['policy']:\n  Configuration for Groot policy wrapper.\n\n  --policy.n_obs_steps int\n                        Basic policy settings (default: 1)\n  --policy.input_features [Dict]\n  --policy.output_features [Dict]\n  --policy.device [str]\n                        e.g. \"cuda\", \"cuda:0\", \"cpu\", or \"mps\" (default: None)\n  --policy.use_amp bool\n                        `use_amp` determines whether to use Automatic Mixed\n                        Precision (AMP) for training and evaluation. With AMP,\n                        automatic gradient scaling is used. (default: False)\n  --policy.use_peft bool\n                        Whether the policy employed PEFT for training.\n                        (default: False)\n  --policy.push_to_hub bool\n                        type: ignore[assignment] # TODO: use a different name\n                        to avoid override (default: True)\n  --policy.repo_id [str]\n  --policy.private [bool]\n                        Upload on private repository on the Hugging Face hub.\n                        (default: None)\n  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n  --policy.license [str]\n                        Add tags to your policy on the hub. (default: None)\n  --policy.pretrained_path [Path]\n                        Either the repo ID of a model hosted on the Hub or a\n                        path to a directory containing weights saved using\n                        `Policy.save_pretrained`. If not provided, the policy\n                        is initialized from scratch. (default: None)\n  --policy.chunk_size int\n  --policy.n_action_steps int\n  --policy.max_state_dim int\n                        Dimension settings (must match pretrained GR00T model\n                        expectations) Maximum state dimension. Shorter states\n                        will be zero-padded. (default: 64)\n  --policy.max_action_dim int\n                        Maximum action dimension. Shorter actions will be\n                        zero-padded. (default: 32)\n  --policy.normalization_mapping Dict\n                        Normalization (start with identity, adjust as needed)\n                        (default: {'VISUAL': <NormalizationMode.IDENTITY:\n                        'IDENTITY'>, 'STATE': <NormalizationMode.MEAN_STD:\n                        'MEAN_STD'>, 'ACTION': <NormalizationMode.MEAN_STD:\n                        'MEAN_STD'>})\n  --policy.image_size int int\n                        Image preprocessing (adjust to match Groot's expected\n                        input) (default: (224, 224))\n  --policy.base_model_path str\n                        Groot-specific model parameters (from\n                        groot_finetune_script.py) Path or HuggingFace model ID\n                        for the base Groot model (default:\n                        nvidia/GR00T-N1.5-3B)\n  --policy.tokenizer_assets_repo str\n                        HF repo ID (or local path) that hosts vocab.json and\n                        merges.txt for Eagle tokenizer. (default:\n                        lerobot/eagle2hg-processor-groot-n1p5)\n  --policy.embodiment_tag str\n                        Embodiment tag to use for training (e.g.\n                        'new_embodiment', 'gr1') (default: new_embodiment)\n  --policy.tune_llm bool\n                        Fine-tuning control arguments Whether to fine-tune the\n                        llm backbone (default: False)\n  --policy.tune_visual bool\n                        Whether to fine-tune the vision tower (default: False)\n  --policy.tune_projector bool\n                        Whether to fine-tune the projector (default: True)\n  --policy.tune_diffusion_model bool\n                        Whether to fine-tune the diffusion model (default:\n                        True)\n  --policy.lora_rank int\n                        LoRA parameters (from groot_finetune_script.py) Rank\n                        for the LORA model. If 0, no LORA will be used.\n                        (default: 0)\n  --policy.lora_alpha int\n                        Alpha value for the LORA model (default: 16)\n  --policy.lora_dropout float\n                        Dropout rate for the LORA model (default: 0.1)\n  --policy.lora_full_model bool\n                        Whether to use the full model for LORA (default:\n                        False)\n  --policy.optimizer_lr float\n                        Training parameters (matching\n                        groot_finetune_script.py) (default: 0.0001)\n  --policy.optimizer_betas float float\n  --policy.optimizer_eps float\n  --policy.optimizer_weight_decay float\n  --policy.warmup_ratio float\n  --policy.use_bf16 bool\n  --policy.video_backend str\n                        Dataset parameters Video backend to use for training\n                        ('decord' or 'torchvision_av') (default: decord)\n  --policy.balance_dataset_weights bool\n                        Whether to balance dataset weights in mixture datasets\n                        (default: True)\n  --policy.balance_trajectory_weights bool\n                        Whether to sample trajectories weighted by their\n                        length (default: True)\n  --policy.dataset_paths [List]\n                        Optional dataset paths for delegating training to\n                        Isaac-GR00T runner (default: None)\n  --policy.output_dir str\n  --policy.save_steps int\n  --policy.max_steps int\n  --policy.batch_size int\n  --policy.dataloader_num_workers int\n  --policy.report_to str\n  --policy.resume bool  \n\nPI0Config ['policy']:\n\n  --policy.n_obs_steps int\n  --policy.input_features [Dict]\n  --policy.output_features [Dict]\n  --policy.device [str]\n                        Device to use for the model (None = auto-detect)\n                        (default: None)\n  --policy.use_amp bool\n                        `use_amp` determines whether to use Automatic Mixed\n                        Precision (AMP) for training and evaluation. With AMP,\n                        automatic gradient scaling is used. (default: False)\n  --policy.use_peft bool\n                        Whether the policy employed PEFT for training.\n                        (default: False)\n  --policy.push_to_hub bool\n                        type: ignore[assignment] # TODO: use a different name\n                        to avoid override (default: True)\n  --policy.repo_id [str]\n  --policy.private [bool]\n                        Upload on private repository on the Hugging Face hub.\n                        (default: None)\n  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n  --policy.license [str]\n                        Add tags to your policy on the hub. (default: None)\n  --policy.pretrained_path [Path]\n                        Either the repo ID of a model hosted on the Hub or a\n                        path to a directory containing weights saved using\n                        `Policy.save_pretrained`. If not provided, the policy\n                        is initialized from scratch. (default: None)\n  --policy.paligemma_variant str\n  --policy.action_expert_variant str\n  --policy.dtype str    Options: \"bfloat16\", \"float32\" (default: float32)\n  --policy.chunk_size int\n                        Number of action steps to predict, in openpi called\n                        \"action_horizon\" (default: 50)\n  --policy.n_action_steps int\n                        Number of action steps to execute (default: 50)\n  --policy.max_state_dim int\n                        Shorter state and action vectors will be padded to\n                        these dimensions (default: 32)\n  --policy.max_action_dim int\n  --policy.num_inference_steps int\n                        Number of denoising steps during inference (default:\n                        10)\n  --policy.time_sampling_beta_alpha float\n  --policy.time_sampling_beta_beta float\n  --policy.time_sampling_scale float\n  --policy.time_sampling_offset float\n  --policy.min_period float\n  --policy.max_period float\n  --policy.image_resolution int int\n  --policy.empty_cameras int\n                        see openpi `preprocessing_pytorch.py` Add empty\n                        images. Used to add empty cameras when no image\n                        features are present. (default: 0)\n  --policy.normalization_mapping Dict\n                        Normalization (default: {'VISUAL':\n                        <NormalizationMode.IDENTITY: 'IDENTITY'>, 'STATE':\n                        <NormalizationMode.MEAN_STD: 'MEAN_STD'>, 'ACTION':\n                        <NormalizationMode.MEAN_STD: 'MEAN_STD'>})\n  --policy.gradient_checkpointing bool\n                        Enable gradient checkpointing for memory optimization\n                        (default: False)\n  --policy.compile_model bool\n                        Whether to use torch.compile for model optimization\n                        (default: False)\n  --policy.compile_mode str\n                        Torch compile mode (default: max-autotune)\n  --policy.freeze_vision_encoder bool\n                        Freeze only the vision encoder (default: False)\n  --policy.train_expert_only bool\n                        Freeze entire VLM, train only action expert and\n                        projections (default: False)\n  --policy.optimizer_lr float\n                        see openpi `CosineDecaySchedule: peak_lr` (default:\n                        2.5e-05)\n  --policy.optimizer_betas float float\n  --policy.optimizer_eps float\n  --policy.optimizer_weight_decay float\n  --policy.optimizer_grad_clip_norm float\n  --policy.scheduler_warmup_steps int\n                        Scheduler settings: see openpi `CosineDecaySchedule`\n                        Note: These will auto-scale if --steps <\n                        scheduler_decay_steps For example, --steps=3000 will\n                        scale warmup to 100 and decay to 3000 (default: 1000)\n  --policy.scheduler_decay_steps int\n  --policy.scheduler_decay_lr float\n  --policy.tokenizer_max_length int\n                        see openpi `__post_init__` (default: 48)\n\nOptional ['policy.rtc_config']:\n  Real-Time Chunking (RTC) configuration\n\nRTCConfig ['policy.rtc_config']:\n  Real-Time Chunking (RTC) configuration\n\n  --policy.rtc_config.enabled bool\n                        Infrastructure (default: False)\n  --policy.rtc_config.prefix_attention_schedule RTCAttentionSchedule\n                        Core RTC settings Todo change to exp (default:\n                        RTCAttentionSchedule.LINEAR)\n  --policy.rtc_config.max_guidance_weight float\n  --policy.rtc_config.execution_horizon int\n  --policy.rtc_config.debug bool\n                        Debug settings (default: False)\n  --policy.rtc_config.debug_maxlen int\n\nPI0FastConfig ['policy']:\n\n  --policy.n_obs_steps int\n  --policy.input_features [Dict]\n  --policy.output_features [Dict]\n  --policy.device [str]\n                        Device to use for the model (None = auto-detect)\n                        (default: None)\n  --policy.use_amp bool\n                        `use_amp` determines whether to use Automatic Mixed\n                        Precision (AMP) for training and evaluation. With AMP,\n                        automatic gradient scaling is used. (default: False)\n  --policy.use_peft bool\n                        Whether the policy employed PEFT for training.\n                        (default: False)\n  --policy.push_to_hub bool\n                        type: ignore[assignment] # TODO: use a different name\n                        to avoid override (default: True)\n  --policy.repo_id [str]\n  --policy.private [bool]\n                        Upload on private repository on the Hugging Face hub.\n                        (default: None)\n  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n  --policy.license [str]\n                        Add tags to your policy on the hub. (default: None)\n  --policy.pretrained_path [Path]\n                        Either the repo ID of a model hosted on the Hub or a\n                        path to a directory containing weights saved using\n                        `Policy.save_pretrained`. If not provided, the policy\n                        is initialized from scratch. (default: None)\n  --policy.paligemma_variant str\n  --policy.action_expert_variant str\n  --policy.dtype str    Options: \"bfloat16\", \"float32\" (default: float32)\n  --policy.chunk_size int\n                        Number of action steps to predict, in openpi called\n                        \"action_horizon\" (default: 50)\n  --policy.n_action_steps int\n                        Number of action steps to execute (default: 50)\n  --policy.max_state_dim int\n                        Shorter state and action vectors will be padded to\n                        these dimensions (default: 32)\n  --policy.max_action_dim int\n  --policy.max_action_tokens int\n  --policy.image_resolution int int\n  --policy.empty_cameras int\n                        see openpi `preprocessing_pytorch.py` Add empty\n                        images. Used to add empty cameras when no image\n                        features are present. (default: 0)\n  --policy.tokenizer_max_length int\n                        see openpi `__post_init__` (default: 200)\n  --policy.text_tokenizer_name str\n  --policy.action_tokenizer_name str\n  --policy.temperature float\n  --policy.max_decoding_steps int\n  --policy.fast_skip_tokens int\n  --policy.validate_action_token_prefix bool\n                        Whether to validate that decoded action tokens start\n                        with \"Action: \" prefix (default: True)\n  --policy.use_kv_cache bool\n                        Whether to use KV cache for faster autoregressive\n                        decoding (default: True)\n  --policy.normalization_mapping Dict\n  --policy.gradient_checkpointing bool\n                        Enable gradient checkpointing for memory optimization\n                        (default: False)\n  --policy.compile_model bool\n                        Whether to use torch.compile for model optimization\n                        (default: False)\n  --policy.compile_mode str\n                        Torch compile mode (default: max-autotune)\n  --policy.optimizer_lr float\n                        see openpi `CosineDecaySchedule: peak_lr` (default:\n                        2.5e-05)\n  --policy.optimizer_betas float float\n  --policy.optimizer_eps float\n  --policy.optimizer_weight_decay float\n  --policy.optimizer_grad_clip_norm float\n  --policy.scheduler_warmup_steps int\n                        Scheduler settings: see openpi `CosineDecaySchedule`\n                        Note: These will auto-scale if --steps <\n                        scheduler_decay_steps For example, --steps=3000 will\n                        scale warmup to 100 and decay to 3000 (default: 1000)\n  --policy.scheduler_decay_steps int\n  --policy.scheduler_decay_lr float\n\nOptional ['policy.rtc_config']:\n  Real-Time Chunking (RTC) configuration\n\nRTCConfig ['policy.rtc_config']:\n  Real-Time Chunking (RTC) configuration\n\n  --policy.rtc_config.enabled bool\n                        Infrastructure (default: False)\n  --policy.rtc_config.prefix_attention_schedule RTCAttentionSchedule\n                        Core RTC settings Todo change to exp (default:\n                        RTCAttentionSchedule.LINEAR)\n  --policy.rtc_config.max_guidance_weight float\n  --policy.rtc_config.execution_horizon int\n  --policy.rtc_config.debug bool\n                        Debug settings (default: False)\n  --policy.rtc_config.debug_maxlen int\n\nPI05Config ['policy']:\n\n  --policy.n_obs_steps int\n  --policy.input_features [Dict]\n  --policy.output_features [Dict]\n  --policy.device [str]\n                        Device to use for the model (None = auto-detect)\n                        (default: None)\n  --policy.use_amp bool\n                        `use_amp` determines whether to use Automatic Mixed\n                        Precision (AMP) for training and evaluation. With AMP,\n                        automatic gradient scaling is used. (default: False)\n  --policy.use_peft bool\n                        Whether the policy employed PEFT for training.\n                        (default: False)\n  --policy.push_to_hub bool\n                        type: ignore[assignment] # TODO: use a different name\n                        to avoid override (default: True)\n  --policy.repo_id [str]\n  --policy.private [bool]\n                        Upload on private repository on the Hugging Face hub.\n                        (default: None)\n  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n  --policy.license [str]\n                        Add tags to your policy on the hub. (default: None)\n  --policy.pretrained_path [Path]\n                        Either the repo ID of a model hosted on the Hub or a\n                        path to a directory containing weights saved using\n                        `Policy.save_pretrained`. If not provided, the policy\n                        is initialized from scratch. (default: None)\n  --policy.paligemma_variant str\n  --policy.action_expert_variant str\n  --policy.dtype str    Options: \"bfloat16\", \"float32\" (default: float32)\n  --policy.chunk_size int\n                        Number of action steps to predict, in openpi called\n                        \"action_horizon\" (default: 50)\n  --policy.n_action_steps int\n                        Number of action steps to execute (default: 50)\n  --policy.max_state_dim int\n                        Shorter state and action vectors will be padded to\n                        these dimensions (default: 32)\n  --policy.max_action_dim int\n  --policy.num_inference_steps int\n                        Flow matching parameters: see openpi `PI0Pytorch`\n                        (default: 10)\n  --policy.time_sampling_beta_alpha float\n  --policy.time_sampling_beta_beta float\n  --policy.time_sampling_scale float\n  --policy.time_sampling_offset float\n  --policy.min_period float\n  --policy.max_period float\n  --policy.image_resolution int int\n  --policy.empty_cameras int\n                        see openpi `preprocessing_pytorch.py` Add empty\n                        images. Used to add empty cameras when no image\n                        features are present. (default: 0)\n  --policy.tokenizer_max_length int\n                        see openpi `__post_init__` (default: 200)\n  --policy.normalization_mapping Dict\n  --policy.gradient_checkpointing bool\n                        Enable gradient checkpointing for memory optimization\n                        (default: False)\n  --policy.compile_model bool\n                        Whether to use torch.compile for model optimization\n                        (default: False)\n  --policy.compile_mode str\n                        Torch compile mode (default: max-autotune)\n  --policy.freeze_vision_encoder bool\n                        Freeze only the vision encoder (default: False)\n  --policy.train_expert_only bool\n                        Freeze entire VLM, train only action expert and\n                        projections (default: False)\n  --policy.optimizer_lr float\n                        see openpi `CosineDecaySchedule: peak_lr` (default:\n                        2.5e-05)\n  --policy.optimizer_betas float float\n  --policy.optimizer_eps float\n  --policy.optimizer_weight_decay float\n  --policy.optimizer_grad_clip_norm float\n  --policy.scheduler_warmup_steps int\n                        Scheduler settings: see openpi `CosineDecaySchedule`\n                        Note: These will auto-scale if --steps <\n                        scheduler_decay_steps For example, --steps=3000 will\n                        scale warmup to 100 and decay to 3000 (default: 1000)\n  --policy.scheduler_decay_steps int\n  --policy.scheduler_decay_lr float\n\nOptional ['policy.rtc_config']:\n  Real-Time Chunking (RTC) configuration\n\nRTCConfig ['policy.rtc_config']:\n  Real-Time Chunking (RTC) configuration\n\n  --policy.rtc_config.enabled bool\n                        Infrastructure (default: False)\n  --policy.rtc_config.prefix_attention_schedule RTCAttentionSchedule\n                        Core RTC settings Todo change to exp (default:\n                        RTCAttentionSchedule.LINEAR)\n  --policy.rtc_config.max_guidance_weight float\n  --policy.rtc_config.execution_horizon int\n  --policy.rtc_config.debug bool\n                        Debug settings (default: False)\n  --policy.rtc_config.debug_maxlen int\n\nSmolVLAConfig ['policy']:\n\n  --policy.n_obs_steps int\n                        Input / output structure. (default: 1)\n  --policy.input_features [Dict]\n  --policy.output_features [Dict]\n  --policy.device [str]\n                        e.g. \"cuda\", \"cuda:0\", \"cpu\", or \"mps\" (default: None)\n  --policy.use_amp bool\n                        `use_amp` determines whether to use Automatic Mixed\n                        Precision (AMP) for training and evaluation. With AMP,\n                        automatic gradient scaling is used. (default: False)\n  --policy.use_peft bool\n                        Whether the policy employed PEFT for training.\n                        (default: False)\n  --policy.push_to_hub bool\n                        type: ignore[assignment] # TODO: use a different name\n                        to avoid override (default: True)\n  --policy.repo_id [str]\n  --policy.private [bool]\n                        Upload on private repository on the Hugging Face hub.\n                        (default: None)\n  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n  --policy.license [str]\n                        Add tags to your policy on the hub. (default: None)\n  --policy.pretrained_path [Path]\n                        Either the repo ID of a model hosted on the Hub or a\n                        path to a directory containing weights saved using\n                        `Policy.save_pretrained`. If not provided, the policy\n                        is initialized from scratch. (default: None)\n  --policy.chunk_size int\n  --policy.n_action_steps int\n  --policy.normalization_mapping Dict\n  --policy.max_state_dim int\n                        Shorter state and action vectors will be padded\n                        (default: 32)\n  --policy.max_action_dim int\n  --policy.resize_imgs_with_padding int int\n                        Image preprocessing (default: (512, 512))\n  --policy.empty_cameras int\n                        Add empty images. Used by smolvla_aloha_sim which adds\n                        the empty left and right wrist cameras in addition to\n                        the top camera. (default: 0)\n  --policy.adapt_to_pi_aloha bool\n                        Converts the joint and gripper values from the\n                        standard Aloha space to the space used by the pi\n                        internal runtime which was used to train the base\n                        model. (default: False)\n  --policy.use_delta_joint_actions_aloha bool\n                        Converts joint dimensions to deltas with respect to\n                        the current state before passing to the model. Gripper\n                        dimensions will remain in absolute values. (default:\n                        False)\n  --policy.tokenizer_max_length int\n                        Tokenizer (default: 48)\n  --policy.num_steps int\n                        Decoding (default: 10)\n  --policy.use_cache bool\n                        Attention utils (default: True)\n  --policy.freeze_vision_encoder bool\n                        Finetuning settings (default: True)\n  --policy.train_expert_only bool\n  --policy.train_state_proj bool\n  --policy.optimizer_lr float\n                        Training presets (default: 0.0001)\n  --policy.optimizer_betas float float\n  --policy.optimizer_eps float\n  --policy.optimizer_weight_decay float\n  --policy.optimizer_grad_clip_norm float\n  --policy.scheduler_warmup_steps int\n  --policy.scheduler_decay_steps int\n  --policy.scheduler_decay_lr float\n  --policy.vlm_model_name str\n                        Select the VLM backbone. (default:\n                        HuggingFaceTB/SmolVLM2-500M-Video-Instruct)\n  --policy.load_vlm_weights bool\n                        Set to True in case of training the expert from\n                        scratch. True when init from pretrained SmolVLA\n                        weights (default: False)\n  --policy.add_image_special_tokens bool\n                        Whether to use special image tokens around image\n                        features. (default: False)\n  --policy.attention_mode str\n  --policy.prefix_length int\n  --policy.pad_language_to str\n                        \"max_length\" (default: longest)\n  --policy.num_expert_layers int\n                        Less or equal to 0 is the default where the action\n                        expert has the same number of layers of VLM. Otherwise\n                        the expert have less layers. (default: -1)\n  --policy.num_vlm_layers int\n                        Number of layers used in the VLM (first num_vlm_layers\n                        layers) (default: 16)\n  --policy.self_attn_every_n_layers int\n                        Interleave SA layers each self_attn_every_n_layers\n                        (default: 2)\n  --policy.expert_width_multiplier float\n                        The action expert hidden size (wrt to the VLM)\n                        (default: 0.75)\n  --policy.min_period float\n                        sensitivity range for the timestep used in sine-cosine\n                        positional encoding (default: 0.004)\n  --policy.max_period float\n\nOptional ['policy.rtc_config']:\n  Real-Time Chunking (RTC) configuration\n\nRTCConfig ['policy.rtc_config']:\n  Real-Time Chunking (RTC) configuration\n\n  --policy.rtc_config.enabled bool\n                        Infrastructure (default: False)\n  --policy.rtc_config.prefix_attention_schedule RTCAttentionSchedule\n                        Core RTC settings Todo change to exp (default:\n                        RTCAttentionSchedule.LINEAR)\n  --policy.rtc_config.max_guidance_weight float\n  --policy.rtc_config.execution_horizon int\n  --policy.rtc_config.debug bool\n                        Debug settings (default: False)\n  --policy.rtc_config.debug_maxlen int\n\nTDMPCConfig ['policy']:\n  Configuration class for TDMPCPolicy.\n  \n      Defaults are configured for training with xarm_lift_medium_replay providing proprioceptive and single\n      camera observations.\n  \n      The parameters you will most likely need to change are the ones which depend on the environment / sensors.\n      Those are: `input_features`, `output_features`, and perhaps `max_random_shift_ratio`.\n  \n      Args:\n          n_action_repeats: The number of times to repeat the action returned by the planning. (hint: Google\n              action repeats in Q-learning or ask your favorite chatbot)\n          horizon: Horizon for model predictive control.\n          n_action_steps: Number of action steps to take from the plan given by model predictive control. This\n              is an alternative to using action repeats. If this is set to more than 1, then we require\n              `n_action_repeats == 1`, `use_mpc == True` and `n_action_steps <= horizon`. Note that this\n              approach of using multiple steps from the plan is not in the original implementation.\n          input_features: A dictionary defining the PolicyFeature of the input data for the policy. The key represents\n              the input data name, and the value is PolicyFeature, which consists of FeatureType and shape attributes.\n          output_features: A dictionary defining the PolicyFeature of the output data for the policy. The key represents\n              the output data name, and the value is PolicyFeature, which consists of FeatureType and shape attributes.\n          normalization_mapping: A dictionary that maps from a str value of FeatureType (e.g., \"STATE\", \"VISUAL\") to\n              a corresponding NormalizationMode (e.g., NormalizationMode.MIN_MAX)\n          image_encoder_hidden_dim: Number of channels for the convolutional layers used for image encoding.\n          state_encoder_hidden_dim: Hidden dimension for MLP used for state vector encoding.\n          latent_dim: Observation's latent embedding dimension.\n          q_ensemble_size: Number of Q function estimators to use in an ensemble for uncertainty estimation.\n          mlp_dim: Hidden dimension of MLPs used for modelling the dynamics encoder, reward function, policy\n              (\u03c0), Q ensemble, and V.\n          discount: Discount factor (\u03b3) to use for the reinforcement learning formalism.\n          use_mpc: Whether to use model predictive control. The alternative is to just sample the policy model\n              (\u03c0) for each step.\n          cem_iterations: Number of iterations for the MPPI/CEM loop in MPC.\n          max_std: Maximum standard deviation for actions sampled from the gaussian PDF in CEM.\n          min_std: Minimum standard deviation for noise applied to actions sampled from the policy model (\u03c0).\n              Doubles up as the minimum standard deviation for actions sampled from the gaussian PDF in CEM.\n          n_gaussian_samples: Number of samples to draw from the gaussian distribution every CEM iteration. Must\n              be non-zero.\n          n_pi_samples: Number of samples to draw from the policy / world model rollout every CEM iteration. Can\n              be zero.\n          uncertainty_regularizer_coeff: Coefficient for the uncertainty regularization used when estimating\n              trajectory values (this is the \u03bb coefficient in eqn 4 of FOWM).\n          n_elites: The number of elite samples to use for updating the gaussian parameters every CEM iteration.\n          elite_weighting_temperature: The temperature to use for softmax weighting (by trajectory value) of the\n              elites, when updating the gaussian parameters for CEM.\n          gaussian_mean_momentum: Momentum (\u03b1) used for EMA updates of the mean parameter \u03bc of the gaussian\n              parameters optimized in CEM. Updates are calculated as \u03bc\u207b \u2190 \u03b1\u03bc\u207b + (1-\u03b1)\u03bc.\n          max_random_shift_ratio: Maximum random shift (as a proportion of the image size) to apply to the\n              image(s) (in units of pixels) for training-time augmentation. If set to 0, no such augmentation\n              is applied. Note that the input images are assumed to be square for this augmentation.\n          reward_coeff: Loss weighting coefficient for the reward regression loss.\n          expectile_weight: Weighting (\u03c4) used in expectile regression for the state value function (V).\n              v_pred < v_target is weighted by \u03c4 and v_pred >= v_target is weighted by (1-\u03c4). \u03c4 is expected to\n              be in [0, 1]. Setting \u03c4 closer to 1 results in a more \"optimistic\" V. This is sensible to do\n              because v_target is obtained by evaluating the learned state-action value functions (Q) with\n              in-sample actions that may not be always optimal.\n          value_coeff: Loss weighting coefficient for both the state-action value (Q) TD loss, and the state\n              value (V) expectile regression loss.\n          consistency_coeff: Loss weighting coefficient for the consistency loss.\n          advantage_scaling: A factor by which the advantages are scaled prior to exponentiation for advantage\n              weighted regression of the policy (\u03c0) estimator parameters. Note that the exponentiated advantages\n              are clamped at 100.0.\n          pi_coeff: Loss weighting coefficient for the action regression loss.\n          temporal_decay_coeff: Exponential decay coefficient for decaying the loss coefficient for future time-\n              steps. Hint: each loss computation involves `horizon` steps worth of actions starting from the\n              current time step.\n          target_model_momentum: Momentum (\u03b1) used for EMA updates of the target models. Updates are calculated\n              as \u03d5 \u2190 \u03b1\u03d5 + (1-\u03b1)\u03b8 where \u03d5 are the parameters of the target model and \u03b8 are the parameters of the\n              model being trained.\n      \n\n  --policy.n_obs_steps int\n                        Input / output structure. (default: 1)\n  --policy.input_features [Dict]\n  --policy.output_features [Dict]\n  --policy.device [str]\n                        e.g. \"cuda\", \"cuda:0\", \"cpu\", or \"mps\" (default: None)\n  --policy.use_amp bool\n                        `use_amp` determines whether to use Automatic Mixed\n                        Precision (AMP) for training and evaluation. With AMP,\n                        automatic gradient scaling is used. (default: False)\n  --policy.use_peft bool\n                        Whether the policy employed PEFT for training.\n                        (default: False)\n  --policy.push_to_hub bool\n                        type: ignore[assignment] # TODO: use a different name\n                        to avoid override (default: True)\n  --policy.repo_id [str]\n  --policy.private [bool]\n                        Upload on private repository on the Hugging Face hub.\n                        (default: None)\n  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n  --policy.license [str]\n                        Add tags to your policy on the hub. (default: None)\n  --policy.pretrained_path [Path]\n                        Either the repo ID of a model hosted on the Hub or a\n                        path to a directory containing weights saved using\n                        `Policy.save_pretrained`. If not provided, the policy\n                        is initialized from scratch. (default: None)\n  --policy.n_action_repeats int\n  --policy.horizon int  \n  --policy.n_action_steps int\n  --policy.normalization_mapping Dict\n  --policy.image_encoder_hidden_dim int\n  --policy.state_encoder_hidden_dim int\n  --policy.latent_dim int\n  --policy.q_ensemble_size int\n  --policy.mlp_dim int  \n  --policy.discount float\n  --policy.use_mpc bool\n  --policy.cem_iterations int\n  --policy.max_std float\n  --policy.min_std float\n  --policy.n_gaussian_samples int\n  --policy.n_pi_samples int\n  --policy.uncertainty_regularizer_coeff float\n  --policy.n_elites int\n  --policy.elite_weighting_temperature float\n  --policy.gaussian_mean_momentum float\n  --policy.max_random_shift_ratio float\n  --policy.reward_coeff float\n  --policy.expectile_weight float\n  --policy.value_coeff float\n  --policy.consistency_coeff float\n  --policy.advantage_scaling float\n  --policy.pi_coeff float\n  --policy.temporal_decay_coeff float\n  --policy.target_model_momentum float\n  --policy.optimizer_lr float\n                        Training presets (default: 0.0003)\n\nVQBeTConfig ['policy']:\n  Configuration class for VQ-BeT.\n  \n      Defaults are configured for training with PushT providing proprioceptive and single camera observations.\n  \n      The parameters you will most likely need to change are the ones which depend on the environment / sensors.\n      Those are: `input_features` and `output_features`.\n  \n      Notes on the inputs and outputs:\n          - \"observation.state\" is required as an input key.\n          - At least one key starting with \"observation.image is required as an input.\n          - If there are multiple keys beginning with \"observation.image\" they are treated as multiple camera\n            views. Right now we only support all images having the same shape.\n          - \"action\" is required as an output key.\n  \n      Args:\n          n_obs_steps: Number of environment steps worth of observations to pass to the policy (takes the\n              current step and additional steps going back).\n          n_action_pred_token: Total number of current token and future tokens that VQ-BeT predicts.\n          action_chunk_size: Action chunk size of each action prediction token.\n          input_features: A dictionary defining the PolicyFeature of the input data for the policy. The key represents\n              the input data name, and the value is PolicyFeature, which consists of FeatureType and shape attributes.\n          output_features: A dictionary defining the PolicyFeature of the output data for the policy. The key represents\n              the output data name, and the value is PolicyFeature, which consists of FeatureType and shape attributes.\n          normalization_mapping: A dictionary that maps from a str value of FeatureType (e.g., \"STATE\", \"VISUAL\") to\n              a corresponding NormalizationMode (e.g., NormalizationMode.MIN_MAX)\n          vision_backbone: Name of the torchvision resnet backbone to use for encoding images.\n          crop_shape: (H, W) shape to crop images to as a preprocessing step for the vision backbone. Must fit\n              within the image size. If None, no cropping is done.\n          crop_is_random: Whether the crop should be random at training time (it's always a center crop in eval\n              mode).\n          pretrained_backbone_weights: Pretrained weights from torchvision to initialize the backbone.\n              `None` means no pretrained weights.\n          use_group_norm: Whether to replace batch normalization with group normalization in the backbone.\n              The group sizes are set to be about 16 (to be precise, feature_dim // 16).\n          spatial_softmax_num_keypoints: Number of keypoints for SpatialSoftmax.\n          n_vqvae_training_steps: Number of optimization steps for training Residual VQ.\n          vqvae_n_embed: Number of embedding vectors in the RVQ dictionary (each layer).\n          vqvae_embedding_dim: Dimension of each embedding vector in the RVQ dictionary.\n          vqvae_enc_hidden_dim: Size of hidden dimensions of Encoder / Decoder part of Residaul VQ-VAE\n          gpt_block_size: Max block size of minGPT (should be larger than the number of input tokens)\n          gpt_input_dim: Size of output input of GPT. This is also used as the dimension of observation features.\n          gpt_output_dim: Size of output dimension of GPT. This is also used as a input dimension of offset / bin prediction headers.\n          gpt_n_layer: Number of layers of GPT\n          gpt_n_head: Number of headers of GPT\n          gpt_hidden_dim: Size of hidden dimensions of GPT\n          dropout: Dropout rate for GPT\n          offset_loss_weight:  A constant that is multiplied to the offset loss\n          primary_code_loss_weight: A constant that is multiplied to the primary code prediction loss\n          secondary_code_loss_weight: A constant that is multiplied to the secondary code prediction loss\n          bet_softmax_temperature: Sampling temperature of code for rollout with VQ-BeT\n          sequentially_select: Whether select code of primary / secondary as sequentially (pick primary code,\n              and then select secodnary code), or at the same time.\n      \n\n  --policy.n_obs_steps int\n  --policy.input_features [Dict]\n  --policy.output_features [Dict]\n  --policy.device [str]\n                        e.g. \"cuda\", \"cuda:0\", \"cpu\", or \"mps\" (default: None)\n  --policy.use_amp bool\n                        `use_amp` determines whether to use Automatic Mixed\n                        Precision (AMP) for training and evaluation. With AMP,\n                        automatic gradient scaling is used. (default: False)\n  --policy.use_peft bool\n                        Whether the policy employed PEFT for training.\n                        (default: False)\n  --policy.push_to_hub bool\n                        type: ignore[assignment] # TODO: use a different name\n                        to avoid override (default: True)\n  --policy.repo_id [str]\n  --policy.private [bool]\n                        Upload on private repository on the Hugging Face hub.\n                        (default: None)\n  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n  --policy.license [str]\n                        Add tags to your policy on the hub. (default: None)\n  --policy.pretrained_path [Path]\n                        Either the repo ID of a model hosted on the Hub or a\n                        path to a directory containing weights saved using\n                        `Policy.save_pretrained`. If not provided, the policy\n                        is initialized from scratch. (default: None)\n  --policy.n_action_pred_token int\n  --policy.action_chunk_size int\n  --policy.normalization_mapping Dict\n  --policy.vision_backbone str\n  --policy.crop_shape [int int]\n  --policy.crop_is_random bool\n  --policy.pretrained_backbone_weights [str]\n  --policy.use_group_norm bool\n  --policy.spatial_softmax_num_keypoints int\n  --policy.n_vqvae_training_steps int\n  --policy.vqvae_n_embed int\n  --policy.vqvae_embedding_dim int\n  --policy.vqvae_enc_hidden_dim int\n  --policy.gpt_block_size int\n  --policy.gpt_input_dim int\n  --policy.gpt_output_dim int\n  --policy.gpt_n_layer int\n  --policy.gpt_n_head int\n  --policy.gpt_hidden_dim int\n  --policy.dropout float\n  --policy.offset_loss_weight float\n  --policy.primary_code_loss_weight float\n  --policy.secondary_code_loss_weight float\n  --policy.bet_softmax_temperature float\n  --policy.sequentially_select bool\n  --policy.optimizer_lr float\n                        Training presets (default: 0.0001)\n  --policy.optimizer_betas Any\n  --policy.optimizer_eps float\n  --policy.optimizer_weight_decay float\n  --policy.optimizer_vqvae_lr float\n  --policy.optimizer_vqvae_weight_decay float\n  --policy.scheduler_warmup_steps int\n\nWallXConfig ['policy']:\n  \n      Configuration class for Wall-X policy.\n  \n      Wall-X is based on Qwen2.5-VL with action prediction capabilities using flow matching.\n      It supports cross-embodiment robotic control through unified action representations.\n  \n      This config supports multi-modal learning with vision, language, and action data.\n      \n\n  --policy.n_obs_steps int\n                        ==================== Input / Output Structure\n                        ==================== (default: 1)\n  --policy.input_features [Dict]\n  --policy.output_features [Dict]\n  --policy.device [str]\n                        e.g. \"cuda\", \"cuda:0\", \"cpu\", or \"mps\" (default: None)\n  --policy.use_amp bool\n                        `use_amp` determines whether to use Automatic Mixed\n                        Precision (AMP) for training and evaluation. With AMP,\n                        automatic gradient scaling is used. (default: False)\n  --policy.use_peft bool\n                        Whether the policy employed PEFT for training.\n                        (default: False)\n  --policy.push_to_hub bool\n                        type: ignore[assignment] # TODO: use a different name\n                        to avoid override (default: True)\n  --policy.repo_id [str]\n  --policy.private [bool]\n                        Upload on private repository on the Hugging Face hub.\n                        (default: None)\n  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n  --policy.license [str]\n                        Add tags to your policy on the hub. (default: None)\n  --policy.pretrained_path [Path]\n                        Either the repo ID of a model hosted on the Hub or a\n                        path to a directory containing weights saved using\n                        `Policy.save_pretrained`. If not provided, the policy\n                        is initialized from scratch. (default: None)\n  --policy.chunk_size int\n                        action_horizon in wall-x (default: 32)\n  --policy.n_action_steps int\n  --policy.max_action_dim int\n                        Action dimension - wall-x uses 20 (default: 20)\n  --policy.max_state_dim int\n                        For proprioception (default: 20)\n  --policy.normalization_mapping Dict\n  --policy.pretrained_name_or_path str\n                        ==================== Action Prediction\n                        ==================== Pretrained model paths (default:\n                        x-square-robot/wall-oss-flow)\n  --policy.action_tokenizer_path [str]\n                        Tokenizer settings (default: physical-\n                        intelligence/fast)\n  --policy.prediction_mode str\n                        Action prediction mode: \"diffusion\" or \"fast\"\n                        (default: diffusion)\n  --policy.attn_implementation str\n                        Attention Implementation, options: \"eager\",\n                        \"flash_attention_2\", \"sdpa\" NOTE: flash-\n                        attn==2.7.4.post1 is required for flash_attention_2\n                        implementation (default: eager)\n  --policy.optimizer_lr float\n                        ==================== Optimizer Presets\n                        ==================== (default: 2e-05)\n  --policy.optimizer_betas float float\n  --policy.optimizer_eps float\n  --policy.optimizer_weight_decay float\n  --policy.optimizer_grad_clip_norm float\n  --policy.scheduler_warmup_steps int\n  --policy.scheduler_decay_steps int\n  --policy.scheduler_decay_lr float\n\nXVLAConfig ['policy']:\n  \n      Configuration class for the XVLA (Extended Vision-Language-Action) policy so it can\n      plug into the LeRobot training stack.\n  \n      The config mirrors the knobs exposed in the original XVLA repository but also\n      declares the input/output feature contract required by LeRobot.\n      \n\n  --policy.n_obs_steps int\n                        Input / output structure (default: 1)\n  --policy.input_features [Dict]\n  --policy.output_features [Dict]\n  --policy.device [str]\n                        e.g. \"cuda\", \"cuda:0\", \"cpu\", or \"mps\" (default: None)\n  --policy.use_amp bool\n                        `use_amp` determines whether to use Automatic Mixed\n                        Precision (AMP) for training and evaluation. With AMP,\n                        automatic gradient scaling is used. (default: False)\n  --policy.use_peft bool\n                        Whether the policy employed PEFT for training.\n                        (default: False)\n  --policy.push_to_hub bool\n                        type: ignore[assignment] # TODO: use a different name\n                        to avoid override (default: True)\n  --policy.repo_id [str]\n  --policy.private [bool]\n                        Upload on private repository on the Hugging Face hub.\n                        (default: None)\n  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n  --policy.license [str]\n                        Add tags to your policy on the hub. (default: None)\n  --policy.pretrained_path [Path]\n                        Either the repo ID of a model hosted on the Hub or a\n                        path to a directory containing weights saved using\n                        `Policy.save_pretrained`. If not provided, the policy\n                        is initialized from scratch. (default: None)\n  --policy.chunk_size int\n  --policy.n_action_steps int\n  --policy.dtype str    Options: \"bfloat16\", \"float32\" (default: float32)\n  --policy.normalization_mapping Dict\n  --policy.florence_config Dict\n                        Florence2 backbone and tokenizer configuration\n                        (default: {})\n  --policy.tokenizer_name str\n  --policy.tokenizer_max_length int\n  --policy.tokenizer_padding_side str\n  --policy.pad_language_to str\n  --policy.hidden_size int\n                        Transformer head (default: 1024)\n  --policy.depth int    \n  --policy.num_heads int\n  --policy.mlp_ratio float\n  --policy.num_domains int\n  --policy.len_soft_prompts int\n  --policy.dim_time int\n  --policy.max_len_seq int\n  --policy.use_hetero_proj bool\n  --policy.action_mode str\n                        Action & proprioception (default: ee6d)\n  --policy.num_denoising_steps int\n  --policy.use_proprio bool\n  --policy.max_state_dim int\n  --policy.max_action_dim int\n                        Maximum action dimension for padding (used by \"auto\"\n                        action mode) (default: 20)\n  --policy.domain_feature_key [str]\n  --policy.resize_imgs_with_padding [int int]\n                        Vision preprocessing (default: None)\n  --policy.num_image_views [int]\n  --policy.empty_cameras int\n  --policy.freeze_vision_encoder bool\n                        Freeze VLM vision encoder weights (default: False)\n  --policy.freeze_language_encoder bool\n                        Freeze VLM language encoder weights (default: False)\n  --policy.train_policy_transformer bool\n                        Allow policy transformer to train (default: True)\n  --policy.train_soft_prompts bool\n                        Allow soft prompts to train (default: True)\n  --policy.optimizer_lr float\n                        Training presets (default: 0.0001)\n  --policy.optimizer_betas float float\n  --policy.optimizer_eps float\n  --policy.optimizer_weight_decay float\n  --policy.optimizer_grad_clip_norm float\n  --policy.optimizer_soft_prompt_lr_scale float\n                        Scale factor for soft-prompt LR (default: 1.0)\n  --policy.optimizer_soft_prompt_warmup_lr_scale [float]\n                        Start scale for warmup (e.g., 0.01) (default: None)\n  --policy.scheduler_warmup_steps int\n  --policy.scheduler_decay_steps int\n  --policy.scheduler_decay_lr float\n\nSACConfig ['policy']:\n  Soft Actor-Critic (SAC) configuration.\n  \n      SAC is an off-policy actor-critic deep RL algorithm based on the maximum entropy\n      reinforcement learning framework. It learns a policy and a Q-function simultaneously\n      using experience collected from the environment.\n  \n      This configuration class contains all the parameters needed to define a SAC agent,\n      including network architectures, optimization settings, and algorithm-specific\n      hyperparameters.\n      \n\n  --policy.n_obs_steps int\n  --policy.input_features [Dict]\n  --policy.output_features [Dict]\n  --policy.device str   Architecture specifics Device to run the model on\n                        (e.g., \"cuda\", \"cpu\") (default: cpu)\n  --policy.use_amp bool\n                        `use_amp` determines whether to use Automatic Mixed\n                        Precision (AMP) for training and evaluation. With AMP,\n                        automatic gradient scaling is used. (default: False)\n  --policy.use_peft bool\n                        Whether the policy employed PEFT for training.\n                        (default: False)\n  --policy.push_to_hub bool\n                        type: ignore[assignment] # TODO: use a different name\n                        to avoid override (default: True)\n  --policy.repo_id [str]\n  --policy.private [bool]\n                        Upload on private repository on the Hugging Face hub.\n                        (default: None)\n  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n  --policy.license [str]\n                        Add tags to your policy on the hub. (default: None)\n  --policy.pretrained_path [Path]\n                        Either the repo ID of a model hosted on the Hub or a\n                        path to a directory containing weights saved using\n                        `Policy.save_pretrained`. If not provided, the policy\n                        is initialized from scratch. (default: None)\n  --policy.normalization_mapping Dict\n                        Mapping of feature types to normalization modes\n                        (default: {'VISUAL': <NormalizationMode.MEAN_STD:\n                        'MEAN_STD'>, 'STATE': <NormalizationMode.MIN_MAX:\n                        'MIN_MAX'>, 'ENV': <NormalizationMode.MIN_MAX:\n                        'MIN_MAX'>, 'ACTION': <NormalizationMode.MIN_MAX:\n                        'MIN_MAX'>})\n  --policy.dataset_stats [Dict]\n                        Statistics for normalizing different types of inputs\n                        (default: {'observation.image': {'mean': [0.485,\n                        0.456, 0.406], 'std': [0.229, 0.224, 0.225]},\n                        'observation.state': {'min': [0.0, 0.0], 'max': [1.0,\n                        1.0]}, 'action': {'min': [0.0, 0.0, 0.0], 'max': [1.0,\n                        1.0, 1.0]}})\n  --policy.storage_device str\n                        Device to store the model on (default: cpu)\n  --policy.vision_encoder_name [str]\n                        Name of the vision encoder model (Set to\n                        \"helper2424/resnet10\" for hil serl resnet10) (default:\n                        None)\n  --policy.freeze_vision_encoder bool\n                        Whether to freeze the vision encoder during training\n                        (default: True)\n  --policy.image_encoder_hidden_dim int\n                        Hidden dimension size for the image encoder (default:\n                        32)\n  --policy.shared_encoder bool\n                        Whether to use a shared encoder for actor and critic\n                        (default: True)\n  --policy.num_discrete_actions [int]\n                        Number of discrete actions, eg for gripper actions\n                        (default: None)\n  --policy.image_embedding_pooling_dim int\n                        Dimension of the image embedding pooling (default: 8)\n  --policy.online_steps int\n                        Training parameter Number of steps for online training\n                        (default: 1000000)\n  --policy.online_buffer_capacity int\n                        Capacity of the online replay buffer (default: 100000)\n  --policy.offline_buffer_capacity int\n                        Capacity of the offline replay buffer (default:\n                        100000)\n  --policy.async_prefetch bool\n                        Whether to use asynchronous prefetching for the\n                        buffers (default: False)\n  --policy.online_step_before_learning int\n                        Number of steps before learning starts (default: 100)\n  --policy.policy_update_freq int\n                        Frequency of policy updates (default: 1)\n  --policy.discount float\n                        SAC algorithm parameters Discount factor for the SAC\n                        algorithm (default: 0.99)\n  --policy.temperature_init float\n                        Initial temperature value (default: 1.0)\n  --policy.num_critics int\n                        Number of critics in the ensemble (default: 2)\n  --policy.num_subsample_critics [int]\n                        Number of subsampled critics for training (default:\n                        None)\n  --policy.critic_lr float\n                        Learning rate for the critic network (default: 0.0003)\n  --policy.actor_lr float\n                        Learning rate for the actor network (default: 0.0003)\n  --policy.temperature_lr float\n                        Learning rate for the temperature parameter (default:\n                        0.0003)\n  --policy.critic_target_update_weight float\n                        Weight for the critic target update (default: 0.005)\n  --policy.utd_ratio int\n                        Update-to-data ratio for the UTD algorithm (If you\n                        want enable utd_ratio, you need to set it to >1)\n                        (default: 1)\n  --policy.state_encoder_hidden_dim int\n                        Hidden dimension size for the state encoder (default:\n                        256)\n  --policy.latent_dim int\n                        Dimension of the latent space (default: 256)\n  --policy.target_entropy [float]\n                        Target entropy for the SAC algorithm (default: None)\n  --policy.use_backup_entropy bool\n                        Whether to use backup entropy for the SAC algorithm\n                        (default: True)\n  --policy.grad_clip_norm float\n                        Gradient clipping norm for the SAC algorithm (default:\n                        40.0)\n  --policy.use_torch_compile bool\n                        Optimizations (default: True)\n\nCriticNetworkConfig ['policy.critic_network_kwargs']:\n  Network configuration\n  Configuration for the critic network architecture\n\n  --policy.critic_network_kwargs.hidden_dims List\n  --policy.critic_network_kwargs.activate_final bool\n  --policy.critic_network_kwargs.final_activation [str]\n\nActorNetworkConfig ['policy.actor_network_kwargs']:\n  Configuration for the actor network architecture\n\n  --policy.actor_network_kwargs.hidden_dims List\n  --policy.actor_network_kwargs.activate_final bool\n\nPolicyConfig ['policy.policy_kwargs']:\n  Configuration for the policy parameters\n\n  --policy.policy_kwargs.use_tanh_squash bool\n  --policy.policy_kwargs.std_min float\n  --policy.policy_kwargs.std_max float\n  --policy.policy_kwargs.init_final float\n\nCriticNetworkConfig ['policy.discrete_critic_network_kwargs']:\n  Configuration for the discrete critic network\n\n  --policy.discrete_critic_network_kwargs.hidden_dims List\n  --policy.discrete_critic_network_kwargs.activate_final bool\n  --policy.discrete_critic_network_kwargs.final_activation [str]\n\nActorLearnerConfig ['policy.actor_learner_config']:\n  Configuration for actor-learner architecture\n\n  --policy.actor_learner_config.learner_host str\n  --policy.actor_learner_config.learner_port int\n  --policy.actor_learner_config.policy_parameters_push_frequency int\n  --policy.actor_learner_config.queue_get_timeout float\n\nConcurrencyConfig ['policy.concurrency']:\n  Configuration for concurrency settings (you can use threads or processes for the actor and learner)\n\n  --policy.concurrency.actor str\n  --policy.concurrency.learner str\n\nRewardClassifierConfig ['policy']:\n  Configuration for the Reward Classifier model.\n\n  --policy.n_obs_steps int\n  --policy.input_features [Dict]\n  --policy.output_features [Dict]\n  --policy.device str   \n  --policy.use_amp bool\n                        `use_amp` determines whether to use Automatic Mixed\n                        Precision (AMP) for training and evaluation. With AMP,\n                        automatic gradient scaling is used. (default: False)\n  --policy.use_peft bool\n                        Whether the policy employed PEFT for training.\n                        (default: False)\n  --policy.push_to_hub bool\n                        type: ignore[assignment] # TODO: use a different name\n                        to avoid override (default: True)\n  --policy.repo_id [str]\n  --policy.private [bool]\n                        Upload on private repository on the Hugging Face hub.\n                        (default: None)\n  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n  --policy.license [str]\n                        Add tags to your policy on the hub. (default: None)\n  --policy.pretrained_path [Path]\n                        Either the repo ID of a model hosted on the Hub or a\n                        path to a directory containing weights saved using\n                        `Policy.save_pretrained`. If not provided, the policy\n                        is initialized from scratch. (default: None)\n  --policy.name str     \n  --policy.num_classes int\n  --policy.hidden_dim int\n  --policy.latent_dim int\n  --policy.image_embedding_pooling_dim int\n  --policy.dropout_rate float\n  --policy.model_name str\n  --policy.model_type str\n                        \"transformer\" or \"cnn\" (default: cnn)\n  --policy.num_cameras int\n  --policy.learning_rate float\n  --policy.weight_decay float\n  --policy.grad_clip_norm float\n  --policy.normalization_mapping Dict\n\nSARMConfig ['policy']:\n  Configuration class for SARM (Stage-Aware Reward Modeling).\n  \n      Supports three annotation modes:\n  \n      1. single_stage (default): No annotations needed. Uses the episode's task description\n         as a single stage covering the entire episode.\n  \n      2. dense_only: Uses dense (fine-grained) annotations from VLM, with an auto-generated\n         single sparse \"task\" stage covering the full episode. The dense head learns detailed\n         subtask progression while sparse provides overall task completion.\n  \n      3. dual: Full dual-head mode with both sparse (high-level) and dense (fine-grained)\n         annotations from VLM. Both heads are trained on their respective annotations.\n  \n      The annotation_mode determines how sparse_temporal_proportions and dense_temporal_proportions\n      are loaded/generated during model initialization.\n      \n\n  --policy.n_obs_steps int\n                        Number of observation history steps (default: 8)\n  --policy.input_features dict\n                        Populated by the processor (video_features,\n                        state_features, text_features) (default: {})\n  --policy.output_features dict\n                        Output features (updated in __post_init__) (default:\n                        {'stage': PolicyFeature(type=<FeatureType.REWARD:\n                        'REWARD'>, shape=(9, 5)), 'progress':\n                        PolicyFeature(type=<FeatureType.REWARD: 'REWARD'>,\n                        shape=(9, 1))})\n  --policy.device [str]\n  --policy.use_amp bool\n                        `use_amp` determines whether to use Automatic Mixed\n                        Precision (AMP) for training and evaluation. With AMP,\n                        automatic gradient scaling is used. (default: False)\n  --policy.use_peft bool\n                        Whether the policy employed PEFT for training.\n                        (default: False)\n  --policy.push_to_hub bool\n                        type: ignore[assignment] # TODO: use a different name\n                        to avoid override (default: True)\n  --policy.repo_id [str]\n  --policy.private [bool]\n                        Upload on private repository on the Hugging Face hub.\n                        (default: None)\n  --policy.tags [List]  Add tags to your policy on the hub. (default: None)\n  --policy.license [str]\n                        Add tags to your policy on the hub. (default: None)\n  --policy.pretrained_path [Path]\n                        Either the repo ID of a model hosted on the Hub or a\n                        path to a directory containing weights saved using\n                        `Policy.save_pretrained`. If not provided, the policy\n                        is initialized from scratch. (default: None)\n  --policy.annotation_mode str\n                        \"single_stage\", \"dense_only\", or \"dual\" (default:\n                        single_stage)\n  --policy.frame_gap int\n                        Frame gap between frames (at 30 fps = 1 second)\n                        (default: 30)\n  --policy.max_rewind_steps int\n                        Maximum rewind steps for temporal augmentation\n                        (default: 4)\n  --policy.image_dim int\n                        Total frames = 1 + n_obs_steps + max_rewind_steps\n                        (computed in property) During training with rewind:\n                        [obs_frames] + [rewind_frames] During inference:\n                        [obs_frames] only Architecture params (default: 512)\n  --policy.text_dim int\n  --policy.hidden_dim int\n  --policy.num_heads int\n  --policy.num_layers int\n  --policy.max_state_dim int\n  --policy.drop_n_last_frames int\n  --policy.batch_size int\n  --policy.clip_batch_size int\n  --policy.dropout float\n  --policy.stage_loss_weight float\n                        Weight for stage classification loss when using\n                        subtask annotations (default: 1.0)\n  --policy.rewind_probability float\n  --policy.language_perturbation_probability float\n  --policy.num_sparse_stages int\n                        Sparse annotations (high-level stages) (default: 1)\n  --policy.sparse_subtask_names [list]\n  --policy.sparse_temporal_proportions [list]\n  --policy.num_dense_stages [int]\n                        Dense annotations (fine-grained stages) (default:\n                        None)\n  --policy.dense_subtask_names [list]\n  --policy.dense_temporal_proportions [list]\n  --policy.pretrained_model_path [str]\n  --policy.image_key str\n                        Key for image used from the dataset (default:\n                        observation.images.top)\n  --policy.state_key str\n  --policy.normalization_mapping Dict",
  "lerobot_eval_help_ok": true,
  "lerobot_import_message": "ok",
  "lerobot_import_ok": true,
  "mujoco_gl_effective": "egl",
  "mujoco_gl_requested": "egl",
  "policy_resolve_ok": true,
  "python_version": "3.10.19 | packaged by conda-forge | (main, Jan 26 2026, 23:45:08) [GCC 14.3.0]",
  "schema_version": "1"
}
